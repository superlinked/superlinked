---
title: "ImageSpace"
description: "Space for generating vector representations from images using SentenceTransformers and OpenCLIP models"
---

## Constructor

```python
ImageSpace(image, model="clip-ViT-B-32", model_cache_dir=None, model_handler=ModelHandler.SENTENCE_TRANSFORMERS, embedding_engine_config=None)
```

Initialize the ImageSpace instance for generating vector representations from images, supporting models from the SentenceTransformers and OpenCLIP projects.

### Parameters

<ParamField
  path="image"
  type="Blob | DescribedBlob | Sequence[Blob | DescribedBlob]"
  required
>
  The image content as a Blob or DescribedBlob (write image+description), or a
  sequence of them.
</ParamField>

<ParamField path="model" type="str" default="clip-ViT-B-32">
  The model identifier for generating image embeddings. Defaults to
  "clip-ViT-B-32".
</ParamField>

<ParamField
  path="model_handler"
  type="ModelHandler"
  default="ModelHandler.SENTENCE_TRANSFORMERS"
>
  The handler for the model, defaults to ModelHandler.SENTENCE_TRANSFORMERS.
</ParamField>

<ParamField path="model_cache_dir" type="Path | None" default="None">
  Directory to cache downloaded models. If None, uses the default cache
  directory. Defaults to None.
</ParamField>

<ParamField
  path="embedding_engine_config"
  type="EmbeddingEngineConfig | None"
  default="None"
>
  Configuration for the embedding engine. Defaults to EmbeddingEngineConfig().
</ParamField>

**Raises**: `InvalidInputException` - If the image and description fields are not from the same schema.

## Inheritance

**Inheritance Chain**:

- `ImageSpace`
- → `Space`
- → `HasTransformationConfig`
- → `HasLength`
- → `Generic`
- → `ABC`

## Properties

<ParamField
  path="transformation_config"
  type="TransformationConfig[Vector, ImageData]"
>
  Configuration for transforming image data into vectors.
</ParamField>
